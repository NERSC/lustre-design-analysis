# NERSC File System Analysis

## Context

### cscratch File System

- Total capacity: 29,763,608,416,864 kibibytes, 6,244,510,656 inodes
- ClusterStor 9000 / Sonexion 2000
    - 5 MDSes in DNE1 configuration
    - Users placed across MDSes once with 4:1 ratio of users on primary:secondary
- Uses and demographics
    - Mounted on [NERSC Edison][] (5,586 nodes) and [NERSC Cori][] (12,076 nodes)
    - General-purpose scratch for application I/O
    - No backups, no assurance of durability
        - Ideally, stores no important data
        - In practice, used for capacity storage as well
    - 20 TB, 10 million inode quota per user by default
    - ad hoc purge of large files
- File system data extracted from Robinhood scan run around November 9, 2018

### project2 File System

- Total capacity: 7,708,795,207,680 kibibytes, 1,200,005,120 inodes
- Uses and demographics
    - Mounted on [NERSC Edison][] (5,586 nodes) and [NERSC Cori][] (12,076 nodes), PDSF (experimental data analysis cluster), and Genepool (bioinformatics cluster)
    - Intended for colder data when accessed from Edison and Cori
    - Used for experimental data working sets when accessed from PDSF and Genepool
    - User-accessible snapshots
    - User-friendly space management features via web portal
    - 1 TB, 1 million inode quota per user by default
    - Not purged
- File system data extracted from GPFS inode scan run on November 9, 2018

### projecta File System

- Total capacity: 5,514,838,671,360 kibibytes, 1,605,004,288 inodes
- Uses and demographics
    - Mounted on [NERSC Edison][] (5,586 nodes) and [NERSC Cori][] (12,076 nodes), PDSF (experimental data analysis cluster), and Genepool (bioinformatics cluster)
    - Sponsored storage file system - users purchase capacity

### projectb File System

- Total capacity: 3,863,205,642,240 kibibytes, 8,000,000,000 inodes
- Uses and demographics
    - Mounted on [NERSC Edison][] (5,586 nodes) and [NERSC Cori][] (12,076 nodes), PDSF (experimental data analysis cluster), and Genepool (bioinformatics cluster)
    - File system used exclusively by the Joint Genome Institute (bioinformatics workloads)
 
## Dataset Provenance

Note that many of the following data source descriptions refer to SQLite
databases.  These databases are _not_ included in the git repository due to size
constraints!  Contact the maintainer of this repository to get the raw SQLite
databases, if available.

### cscratch\_daily\_iorates.csv

This data is derived from a database generated for the TOKIO Data DAG project
using its *update\_daily\_db.py* tool.  That tool ultimately calls the
following code which is dependent upon [pytokio][]:

```python
def load_tts(self, system):
    totals = {}
    success = False
    for direction in 'read', 'write':
        tmp_df = tokio.tools.hdf5.get_dataframe_from_time_range(
            fsname=system,
            dataset_name='datatargets/%sbytes' % direction,
            datetime_start=self.date,
            datetime_end=(self.date + datetime.timedelta(days=1, seconds=-1)),
            fix_errors=True)

        if tmp_df is not None:
            totals[direction] = tmp_df.sum().sum()
            success |= True
        else:
            totals[direction] = -0.0

    self._store_rw_bytes(system, totals['read'], totals['write'])

    return success
```

The code essentially takes the entire sum of each TTS file's
`datatargets/readbytes` and `datatargets/writebytes` datasets as the daily
totals, then builds a list of dates and their total traffic.

To generate the literal CSV, the following was run:

    sqlite3 -header -separator , ../tokio-datadag/daily_io_traffic.db "select * from cscratch order by date" > cscratch_daily_iorates.csv

### isdct\_summary\_20190401.csv

Generated using the [pytokio ISDCT caching tool][], e.g.,

    cache_isdct -c /path/to/Intel_DCT_20190401.tgz > isdct_summary_20190401.csv

### Lustre inode data

Data was generated from Robinhood; `mysqldump` was run to dump the contents of
the _entries_ and _names_ tables, which were saved to the following files:

    /global/cscratch1/cpurge01/nersc/log/sqldump/cpurge01_20181109_entries.sql
    /global/cscratch1/cpurge01/nersc/log/sqldump/cpurge01_20181109_names.sql

### GPFS inode data

Data is generated by the [nightly inode scan](gpfsinodedumpdocs).

### Lustre SQLite databases

[mysqldump2csv][] was run on each `.sql` file dumped from Robinhood's database
to generate CSV representations of the raw data.  These files became

    cpurge01_20181109_entries.csv
    cpurge01_20181109_names.csv

Generating these CSVs took the better part of a calendar day.  It became clear
that these CSVs would not be manageable, so they were then converted into a
SQLite database.

The following files contain the schemata used to generate the `entries` and
`names` tables:

    cpurge01_20181109_entries.sqlite_schema
    cpurge01_20181109_names.sqlite_schema

Once the tables were created, the following was performed to ingest the CSVs:

    sqlite> pragma journal_mode=off; pragma synchronous=off; pragma locking_mode=exclusive;
    sqlite> .mode csv
    sqlite> .import cpurge01_20181109_entries.csv entries
    sqlite> .import cpurge01_20181109_names.csv names

This takes a few hours, but is much faster than the process of converting the
mysqldump output to CSV.

This process resulted in `cscratch_20181109.sqlite` which contains two tables:
`entries` and `names`.

### GPFS SQLite databases

Create a CSV representation of the raw data:

    $ awk '{printf("%d,%d,%d,%d,%s,%d,%s,%d,%d,%d,%d\n",
                   $1, $2, $3, $4, $7, $8, $11, $12, $13, $14, $15);}' \
        /project/statistics/LIST/tlprojecta/2018-11-09/mmapplypolicy.24028.8DCA8782.list.allfiles \
        > tlprojecta_20181109_entries.csv

This can take an hour or two.  Then create the SQLite database:

    $ sqlite3 tlprojecta_20181109.sqlite
    sqlite> CREATE TABLE ENTRIES (
              inode integer,
              generation integer,
              snapshot integer,
              size integer,
              misc_attrs text,
              nlinks integer,
              mode text,
              atime integer,
              mtime integer,
              blocksize integer,
              ctime integer
            );
    sqlite> pragma journal_mode=off; pragma synchronous=off; pragma locking_mode=exclusive;
    sqlite> .mode csv
    sqlite> .import tlprojecta_20181109_entries.csv entries


### SQLite databases containing only inode sizes

For analyses that only require inode size information, it is helpful to create a
database that contains _only_ the inode sizes.  Because different types of
inodes may be stored differently (e.g., a file inode's size reflects the size of
the data to which it points on Lustre OSTs, whereas a directory inode's size
reflects capacity taken up on the MDTs), separating sizes out by inode type is
also beneficial.

The _makesizetables.py_ script effectively does this and separates out every
inode's size into an inode type-specific table.

    $ ./makesizetables.py -v -f cscratch_20181109.sqlite cscratch_20181109_sizebytype.sqlite

Where

* `-v` enables debugging messages (optional)
* `-f` disables journaling, synchronous writes, and locking
* `cscratch_20181109.sqlite` is the existing input SQLite database with an _entries_ table
* `cscratch_20181109_sizebytype.sqlite` is the SQLite database to create and populate with the demultiplexed output

This script does the equivalent of

    sqlite> .open "cscratch_20181109.sqlite" as main;
    sqlite> attach database "cscratch_20181109_sizebytype.sqlite" as sizes;
    sqlite> pragma journal_mode=off; pragma synchronous=off; pragma locking_mode=exclusive;
    sqlite> CREATE TABLE sizes.files (size integer);
    sqlite> INSERT INTO sizes.files(size) SELECT size FROM main.entries WHERE entries.type == "file";
    sqlite> CREATE INDEX sizes.file_size_idx ON files (size)
    ...
    sqlite> CREATE TABLE sizes.dirs (size integer);
    sqlite> INSERT INTO sizes.dirs(size) SELECT size FROM main.entries WHERE entries.type == "dir";
    sqlite> CREATE INDEX sizes.dir_size_idx ON dirs (size)
    ...

The script works on _entries_ tables that have either the Lustre/Robinhood
or GPFS/mmapplypolicy schemata.  For speed, it's probably a good idea to
create indices on the _entries.type_ column (Lustre) or _entries.mode_
column (GPFS).

The resulting `cscratch_20181109_sizebytype.sqlite` database is an order of
magnitude smaller in size than the full database, is anonymous, and can
feasibly be shared with third parties.

### Histograms of file sizes

If operating on the entire _entries_ table, create an index on size:

    sqlite> CREATE INDEX size_index ON entries (size);

This takes time.  Then run

    $ ./histogram.py cscratch_20181109.sqlite > cscratch_20181109_hist.csv

to generate a CSV-formatted output.

To generate size histograms for each inode type,

    $ ./histogram.py -t files,dirs,symlinks,blks,chrs,fifos,socks cscratch_20181109_sizebytype.sqlite \
        > tlproject2_20181109_sizebytype_hist.csv

Recall that the _*\_sizebytype.sqlite_ databases created by _makesizetables.py_
already contains indexes on size.

### Directory size distributions

This is a very expensive process because it involves joining the two tables
produced by Robinhood.  

After disabling all the SQLite consistency stuff to get
the maximum insert performance, create the table where the joined results will
be output.  The schema for this directory distribution master table should be:

    CREATE TABLE dirdist (
      parent_id text,
      id text,
      size integer,
      type text
    );

Then populate the directory distribution table:

    INSERT INTO dirdist 
      SELECT
        names.parent_id,
        entries.id,
        entries.size,
        entries.type
      FROM
        entries
      INNER JOIN
        names ON entries.id = names.id;
        
The above JOIN will take a very long time.  Run it overnight or over a weekend.
I did this on an Optane drive and it still took ~18 hours.  It may have been
faster with indexes on both sides of the JOIN.

Here are exact instructions on doing the above:

    $ sqlite3 cscratch_20181109.sqlite
    sqlite> pragma journal_mode=off; pragma synchronous=off; pragma locking_mode=exclusive;
    off
    exclusive
    sqlite> attach database "cscratch_20181109_dirdist.sqlite" as dd;
    sqlite> CREATE TABLE dd.dirdist(parent_id TEXT, id TEXT, size INTEGER, type TEXT);
    sqlite> INSERT INTO dd.dirdist SELECT names.parent_id, entries.id, entries.size, entries.type FROM entries INNER JOIN names ON entries.id = names.id;

Getting child counts:

    CREATE TABLE child_counts (
            parent_id text,
            count integer
    );
    INSERT INTO child_counts 
        SELECT parent_id, count(id) FROM dirdist GROUP BY parent_id;
    CREATE INDEX count_idx on child_counts (count);

This table will have every parent (all of whom should be of `type = 'dir'`) and
every inode within that directory.  Then it can be converted into a histogram
using the `histogram.py` tool:

    ./histogram.py -t child_counts -c count cpurge01_20181109_dirdist.db

[pytokio]: https://pytokio.readthedocs.io/en/latest/
[pytokio ISDCT caching tool]: https://pytokio.readthedocs.io/en/latest/api/tokio.cli.cache_isdct.html
[NERSC Edison]: http://www.nersc.gov/users/computational-systems/edison/configuration/
[NERSC Cori]: http://www.nersc.gov/users/computational-systems/cori/configuration/
[mysqldump2csv]: https://github.com/jamesmishra/mysqldump-to-csv
[gpfsinodedumpdocs]: https://sites.google.com/a/lbl.gov/glennklockwood/holistic-i-o-characterization/gpfs-inode-dumps

[![Build Status](https://travis-ci.org/glennklockwood/lustre-design-analysis.svg?branch=master)](https://travis-ci.org/glennklockwood/lustre-design-analysis)
[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.3244452.svg)](https://doi.org/10.5281/zenodo.3244452)
